# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dBv2MPxbmaD8p0k_imAFkS5RY2ZawCuj
"""

import numpy as np

#Initializing the network parameters
inputsNodes= 5
hiddenNodes= 10
outputNodes= 3
learningRate = 0.1

#Defining sigmoid activation function
def sigmoid(z):
  return 1/(1 + np.exp(-z))
#Defining sigmoid function derivative
def sigmoid_derivative(z):
  return z * (1-z)

np.random.seed(0)
inputweightsHidden = np.ones((inputsNodes, hiddenNodes))
hiddenweightoutput= np.ones((hiddenNodes, outputNodes))
biasHidden = np.ones((1, hiddenNodes))
biasOutput = np.ones((1, outputNodes))

#Defining the function for feed forward
def feedforward(input):
  hiddenInput = np.dot(input,inputweightsHidden ) + biasHidden
  hiddenOutput = sigmoid(hiddenInput)
  finalInput = np.dot(hiddenOutput, hiddenweightoutput) + biasOutput
  finalOutput = sigmoid(finalInput)
  return hiddenOutput, finalOutput

# Defining function to compute the sum of squares loss method
def sum_of_squares_loss(y , t):
  return 0.5 * np.sum((y - t) ** 2)

#Defining the function for backpropagation
def backpropagation( input , hiddenOutput, output , target):
    global inputweightsHidden, hiddenweightoutput, biasHidden, biasOutput

    outputError = output - target
    outputDelta = outputError * output * (1 - output)

    hiddenError = np.dot(outputDelta, hiddenweightoutput.T)
    hiddenDelta = hiddenError * hiddenOutput * (1 - hiddenOutput)

    inputweightsHidden -= learningRate * np.dot(input.T, hiddenDelta)
    hiddenweightoutput -= learningRate * np.dot(hiddenOutput.T, outputDelta)

    biasHidden -= learningRate * np.sum(hiddenDelta, axis=0 , keepdims = True)
    biasOutput -= learningRate * np.sum(outputDelta, axis=0 , keepdims = True)

#Reading the input from the user
inputData = [float(input()) for _ in range(8)]
inputs = np.array(inputData[:5]).reshape(1,-1)
targets = np.array(inputData[5:]).reshape(1,-1)

#Computing the initial loss of the network
hiddenOutput , output = feedforward(inputs)
initial_loss =  sum_of_squares_loss(output, targets)

#Backpropagation
backpropagation(inputs, hiddenOutput, output , targets)

#Computing the loss after training
hiddenOutput , output = feedforward(inputs)
final_loss = sum_of_squares_loss(output, targets)

#Printing the values to 4 decimal places
print(f"{initial_loss:.4f}")
print(f"{final_loss:.4f}")